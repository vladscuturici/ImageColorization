{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install deeplake --quiet"
      ],
      "metadata": {
        "id": "5HhL3mGPZ9Hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import deeplake\n",
        "print(deeplake.__version__)"
      ],
      "metadata": {
        "id": "Qe0zXd0FZ-Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "8zku-eV6Z_wC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "places205_ds = deeplake.query('SELECT * FROM \"hub://activeloop/places205\" LIMIT 20000')\n",
        "\n",
        "places205_train_loader = places205_ds.pytorch()\n",
        "\n",
        "for i, sample in enumerate(places205_train_loader):\n",
        "    if i >= 10:\n",
        "        break\n",
        "    img = sample['images']\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"Image shape: {img.shape}\")\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xHSi02J0aBHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(places205_train_loader))"
      ],
      "metadata": {
        "id": "Gn3eZ3yyaDUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " coco_ds = deeplake.query('SELECT * FROM \"hub://activeloop/coco-train\" LIMIT 70000')\n",
        "\n",
        "coco_train_loader = coco_ds.pytorch()\n",
        "\n",
        "for i, sample in enumerate(coco_train_loader):\n",
        "    if i >= 10:\n",
        "        break\n",
        "    img = sample['images']\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"Image shape: {img.shape}\")\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "5lobeohsaFHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(coco_train_loader))"
      ],
      "metadata": {
        "id": "DCwV-35faGem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "Dxx4OPeZaHMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_tensor = transforms.ToTensor()\n",
        "to_pil = transforms.ToPILImage()\n",
        "resize = transforms.Resize((256, 256))\n",
        "\n",
        "def preprocess_image(img):\n",
        "    if isinstance(img, torch.Tensor):\n",
        "        img = to_pil(img)\n",
        "\n",
        "    if isinstance(img, np.ndarray):\n",
        "        img = np.squeeze(img)\n",
        "        if img.ndim == 2 or img.ndim == 3:\n",
        "            img = Image.fromarray(img.astype(np.uint8))\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported image shape for PIL conversion: {img.shape}\")\n",
        "\n",
        "    img = resize(img)\n",
        "\n",
        "    img_np = np.array(img)\n",
        "\n",
        "    if img_np.ndim == 2:\n",
        "        img_np = np.stack([img_np]*3, axis=-1)\n",
        "    elif img_np.shape[2] == 1:\n",
        "        img_np = np.repeat(img_np, 3, axis=2)\n",
        "\n",
        "    lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB).astype(np.float32)\n",
        "    L = lab[:, :, 0:1] / 255.0\n",
        "    ab = (lab[:, :, 1:3] - 128) / 128.0\n",
        "\n",
        "    L_tensor = torch.from_numpy(L).permute(2, 0, 1).float()\n",
        "    ab_tensor = torch.from_numpy(ab).permute(2, 0, 1).float()\n",
        "\n",
        "    return L_tensor, ab_tensor\n"
      ],
      "metadata": {
        "id": "X0ufyjMjaIDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "Xaz2K5rvaJLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import ImageOps\n",
        "\n",
        "class ColorizationDataset(Dataset):\n",
        "    def __init__(self, deeplake_dataset, style_encoder=None, device='cpu', test=0):\n",
        "        self.dataset = deeplake_dataset\n",
        "        self.style_encoder = style_encoder\n",
        "        self.device = device\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "        self.resize_224 = transforms.Resize((224, 224))\n",
        "        if test == 0:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(256, scale=(0.8, 1.0)),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomVerticalFlip(),\n",
        "                transforms.RandomRotation(30),\n",
        "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "\n",
        "        else:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((256, 256)),\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.dataset[idx]\n",
        "        img = sample['images']\n",
        "\n",
        "        if isinstance(img, torch.Tensor):\n",
        "            img_np = img.permute(1, 2, 0).cpu().numpy()\n",
        "        else:\n",
        "            img_np = np.array(img)\n",
        "\n",
        "        img_np = np.squeeze(img_np)\n",
        "\n",
        "        if img_np.ndim == 2:\n",
        "            img_np = np.stack([img_np] * 3, axis=-1)\n",
        "        elif img_np.ndim == 3 and img_np.shape[2] == 1:\n",
        "            img_np = np.repeat(img_np, 3, axis=2)\n",
        "\n",
        "        img_np = img_np.astype(np.uint8)\n",
        "        img_pil = ImageOps.exif_transpose(Image.fromarray(img_np))\n",
        "\n",
        "        img_aug = self.transform(img_pil)\n",
        "        rgb_tensor = img_aug.unsqueeze(0).to(self.device)\n",
        "\n",
        "        style_feats = None\n",
        "        if self.style_encoder:\n",
        "            with torch.no_grad():\n",
        "                features = self.style_encoder(rgb_tensor)\n",
        "                style_feats = torch.mean(features, dim=[2, 3])\n",
        "\n",
        "        L, ab = preprocess_image(img_aug)\n",
        "\n",
        "        return {\n",
        "            'L': L,\n",
        "            'ab': ab,\n",
        "            'style_feats': style_feats.squeeze(0) if style_feats is not None else None\n",
        "        }\n"
      ],
      "metadata": {
        "id": "a50ctiz2aKBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torchvision.models import resnet50\n",
        "from einops import rearrange"
      ],
      "metadata": {
        "id": "MnDt9iV2aLSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "L0tZ5UTbaMK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "resnet_style = models.resnet50(pretrained=True)\n",
        "resnet_style = torch.nn.Sequential(*list(resnet_style.children())[:-1])\n",
        "resnet_style.eval().to(device)"
      ],
      "metadata": {
        "id": "-pbOGsORaNGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ColorQueryDecoder(nn.Module):\n",
        "    def __init__(self, feature_dim, num_queries=64):\n",
        "        super().__init__()\n",
        "        self.queries = nn.Parameter(torch.randn(num_queries, feature_dim))\n",
        "        self.transformer = nn.Transformer(d_model=feature_dim, batch_first=True, dropout=0.1)\n",
        "        self.pos_encoding = nn.Parameter(torch.randn(64, 1, feature_dim))\n",
        "        self.query_norm = nn.LayerNorm(feature_dim)\n",
        "\n",
        "    def forward(self, features):\n",
        "        B, C, H, W = features.shape\n",
        "        x = features.flatten(2).permute(0, 2, 1)\n",
        "        x = x + self.pos_encoding[:x.size(0)]\n",
        "        queries = self.queries.unsqueeze(0).expand(B, -1, -1)\n",
        "\n",
        "        color_features = self.transformer(queries, x)\n",
        "        color_features = self.query_norm(color_features)\n",
        "        color_features = F.dropout(color_features, p=0.1, training=self.training)\n",
        "\n",
        "        return color_features"
      ],
      "metadata": {
        "id": "fx6Kt2DDaN4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SPADE(nn.Module):\n",
        "    def __init__(self, norm_nc, label_nc):\n",
        "        super().__init__()\n",
        "        self.norm = nn.BatchNorm2d(norm_nc, affine=False)\n",
        "        nhidden = 128\n",
        "        self.mlp_shared = nn.Sequential(\n",
        "            nn.Conv2d(label_nc, nhidden, 3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.mlp_gamma = nn.Conv2d(nhidden, norm_nc, 3, padding=1)\n",
        "        self.mlp_beta = nn.Conv2d(nhidden, norm_nc, 3, padding=1)\n",
        "\n",
        "    def forward(self, x, segmap):\n",
        "        segmap = segmap.to(x.device)\n",
        "        normalized = self.norm(x)\n",
        "        segmap = nn.functional.interpolate(segmap, size=x.size()[2:], mode='nearest')\n",
        "        actv = self.mlp_shared(segmap)\n",
        "        gamma = self.mlp_gamma(actv)\n",
        "        beta = self.mlp_beta(actv)\n",
        "        out = normalized * (1 + gamma) + beta\n",
        "        return out"
      ],
      "metadata": {
        "id": "SgUpe-ubaPBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InstanceFusion(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels * 2, in_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, global_feats, instance_feats):\n",
        "        combined = torch.cat([global_feats, instance_feats], dim=1)\n",
        "        return self.conv(combined)"
      ],
      "metadata": {
        "id": "yrRfNuRfaQBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PixelDecoder(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.up4 = nn.ConvTranspose2d(in_channels, 512, 4, 2, 1)\n",
        "        self.up3 = nn.ConvTranspose2d(512 + 1024, 256, 4, 2, 1)\n",
        "        self.up2 = nn.ConvTranspose2d(256 + 512, 128, 4, 2, 1)\n",
        "        self.out = nn.Conv2d(128, 2, 3, padding=1)\n",
        "\n",
        "    def forward(self, x4, x3, x2):\n",
        "        x = self.up4(x4)\n",
        "        x = torch.cat([x, x3], dim=1)\n",
        "        x = self.up3(x)\n",
        "        x = torch.cat([x, x2], dim=1)\n",
        "        x = self.up2(x)\n",
        "        out = torch.tanh(self.out(x))\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "DVrHtlW7aULU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "aoE3VW8KaVh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ColorizationNet(nn.Module):\n",
        "    def __init__(self, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        resnet = resnet50(pretrained=True)\n",
        "        resnet = resnet.to(self.device)\n",
        "        self.encoder = nn.ModuleDict({\n",
        "            \"layer1\": nn.Sequential(*list(resnet.children())[:5]),\n",
        "            \"layer2\": resnet.layer2,\n",
        "            \"layer3\": resnet.layer3,\n",
        "            \"layer4\": resnet.layer4,\n",
        "        })\n",
        "\n",
        "        self.encoder.to(self.device)\n",
        "\n",
        "        self.pixel_decoder = PixelDecoder(2048).to(self.device)\n",
        "        self.color_query_decoder = ColorQueryDecoder(2048).to(self.device)\n",
        "        self.spade = SPADE(2048, 1).to(self.device)\n",
        "        self.instance_fusion = InstanceFusion(2048).to(self.device)\n",
        "        self.guidance_proj = nn.Linear(2048, 2048).to(self.device)\n",
        "        self.instance_gate = nn.Conv2d(2048, 2048, kernel_size=1)\n",
        "        self.query_gate = nn.Conv2d(2048, 2048, kernel_size=1).to(self.device)\n",
        "\n",
        "    def forward(self, x_gray, segmap, instance_feats=None, style_feats=None):\n",
        "        x1 = self.encoder[\"layer1\"](x_gray)\n",
        "        x2 = self.encoder[\"layer2\"](x1)\n",
        "        x3 = self.encoder[\"layer3\"](x2)\n",
        "        x4 = self.encoder[\"layer4\"](x3)\n",
        "\n",
        "        x4 = self.spade(x4, segmap)\n",
        "\n",
        "        if instance_feats is not None:\n",
        "            if instance_feats.shape[1] == 1:\n",
        "                instance_feats = instance_feats.repeat(1, x4.shape[1], 1, 1)\n",
        "            if instance_feats.shape[2:] != x4.shape[2:]:\n",
        "                instance_feats = F.interpolate(instance_feats, size=x4.shape[2:], mode='bilinear', align_corners=False)\n",
        "            gate = torch.sigmoid(self.instance_gate(x4))\n",
        "            x4 = self.instance_fusion(x4, instance_feats * gate)\n",
        "\n",
        "        color_query = self.color_query_decoder(x4)\n",
        "\n",
        "        query_map = self.guidance_proj(color_query)\n",
        "        query_map = query_map.permute(0, 2, 1).contiguous()\n",
        "        query_map = query_map.view(x4.size(0), x4.size(1), x4.size(2), x4.size(3))\n",
        "\n",
        "        query_map = query_map / (query_map.norm(dim=1, keepdim=True) + 1e-6)\n",
        "        query_map = F.dropout(query_map, p=0.3, training=self.training)\n",
        "\n",
        "        gate = torch.sigmoid(self.query_gate(x4))\n",
        "        x4 = x4 + 0.02 * gate * query_map\n",
        "\n",
        "        ab_channels = self.pixel_decoder(x4, x3, x2)\n",
        "        return ab_channels, color_query"
      ],
      "metadata": {
        "id": "zLlYVPJraWeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import vgg16\n",
        "from torchvision.models.feature_extraction import create_feature_extractor"
      ],
      "metadata": {
        "id": "oKkCUO9_aXts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, perceptual_weight=0.7, l1_weight=0.8, colorfulness_weight=0.4, query_loss_weight=0.3, centering_weight=0.01, histogram_weight=0.2):\n",
        "        super().__init__()\n",
        "        self.l1_loss = nn.L1Loss()\n",
        "        self.perceptual_weight = perceptual_weight\n",
        "        self.l1_weight = l1_weight\n",
        "        self.colorfulness_weight = colorfulness_weight\n",
        "        self.query_loss_weight = query_loss_weight\n",
        "        self.centering_weight = centering_weight\n",
        "        self.histogram_weight = histogram_weight\n",
        "\n",
        "        vgg = vgg16(pretrained=True).features.eval()\n",
        "        self.perceptual_extractor = create_feature_extractor(\n",
        "            vgg,\n",
        "            return_nodes={\n",
        "                \"3\": \"relu1_2\",\n",
        "                \"8\": \"relu2_2\",\n",
        "                \"15\": \"relu3_3\",\n",
        "                \"22\": \"relu4_3\"\n",
        "            }\n",
        "        )\n",
        "        for param in self.perceptual_extractor.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def compute_histogram_loss(self, pred_ab, target_ab, bins=32):\n",
        "        B, _, H, W = pred_ab.shape\n",
        "        loss = 0.0\n",
        "\n",
        "        for c in range(2):\n",
        "            pred_hist = []\n",
        "            target_hist = []\n",
        "\n",
        "            for i in range(B):\n",
        "                pred_vals = pred_ab[i, c].flatten()\n",
        "                target_vals = target_ab[i, c].flatten()\n",
        "\n",
        "                pred_h = torch.histc(pred_vals.float(), bins=bins, min=-1.0, max=1.0)\n",
        "                target_h = torch.histc(target_vals.float(), bins=bins, min=-1.0, max=1.0)\n",
        "\n",
        "                pred_h = pred_h / (pred_h.sum() + 1e-6)\n",
        "                target_h = target_h / (target_h.sum() + 1e-6)\n",
        "\n",
        "                pred_hist.append(pred_h)\n",
        "                target_hist.append(target_h)\n",
        "\n",
        "            pred_hist = torch.stack(pred_hist)\n",
        "            target_hist = torch.stack(target_hist)\n",
        "\n",
        "            loss += nn.functional.mse_loss(pred_hist, target_hist)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def forward(self, pred_ab, target_ab, input_l, color_query=None, target_query=None):\n",
        "        pred_ab_upsampled = nn.functional.interpolate(pred_ab, size=input_l.shape[2:], mode='bilinear', align_corners=False)\n",
        "        target_ab_upsampled = nn.functional.interpolate(target_ab, size=input_l.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        l1_loss_val = self.l1_loss(pred_ab_upsampled, target_ab_upsampled)\n",
        "        loss = self.l1_weight * l1_loss_val\n",
        "\n",
        "        pred_lab = torch.cat([input_l, pred_ab_upsampled], dim=1)\n",
        "        target_lab = torch.cat([input_l, target_ab_upsampled], dim=1)\n",
        "\n",
        "        pred_feats = self.perceptual_extractor(pred_lab)\n",
        "        target_feats = self.perceptual_extractor(target_lab)\n",
        "\n",
        "        perceptual_loss_val = sum(nn.functional.mse_loss(pred_feats[k], target_feats[k]) for k in pred_feats)\n",
        "        loss += self.perceptual_weight * perceptual_loss_val\n",
        "\n",
        "        ab_flat = pred_ab.view(pred_ab.size(0), 2, -1)\n",
        "        std_ab = ab_flat.std(dim=2).mean(dim=1)\n",
        "        colorfulness_loss_val = torch.relu(0.5 - std_ab).mean()\n",
        "        loss += self.colorfulness_weight * colorfulness_loss_val\n",
        "\n",
        "        query_loss_val = 0.0\n",
        "        if color_query is not None and target_query is not None:\n",
        "            query_loss_val = nn.functional.mse_loss(color_query, target_query)\n",
        "            loss += self.query_loss_weight * query_loss_val\n",
        "\n",
        "        a_mean = pred_ab[:, 0].mean()\n",
        "        ab_centering_loss_val = torch.abs(a_mean)\n",
        "        loss += self.centering_weight * ab_centering_loss_val\n",
        "\n",
        "        histogram_loss_val = self.compute_histogram_loss(pred_ab, target_ab)\n",
        "        loss += self.histogram_weight * histogram_loss_val\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            print(\"=== NaN detected in loss ===\")\n",
        "            print(\"L1 Loss:\", l1_loss_val.item())\n",
        "            print(\"Perceptual Loss:\", perceptual_loss_val.item())\n",
        "            print(\"Colorfulness Loss:\", colorfulness_loss_val.item())\n",
        "            print(\"Query Loss:\", query_loss_val.item() if isinstance(query_loss_val, torch.Tensor) else query_loss_val)\n",
        "            print(\"Centering Loss:\", ab_centering_loss_val.item())\n",
        "            print(\"Histogram Loss:\", histogram_loss_val.item())\n",
        "            print(\"Total Loss: NaN\")\n",
        "            print(\"===========================\")\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "_v5YhVJDaZdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import gzip\n",
        "import pickle\n",
        "import os"
      ],
      "metadata": {
        "id": "98rAts-Aaba2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"places_images_only\", exist_ok=True)\n",
        "\n",
        "places_samples = 20000\n",
        "\n",
        "\n",
        "for i, sample in enumerate(tqdm(places205_train_loader, total=places_samples, desc=\"Saving resized Places images\")):\n",
        "    if i >= places_samples:\n",
        "        break\n",
        "\n",
        "    img = sample['images']\n",
        "\n",
        "    if isinstance(img, torch.Tensor):\n",
        "        img = img.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "    img = img.astype(np.uint8)\n",
        "\n",
        "    if (\n",
        "        img.ndim != 3 or\n",
        "        img.shape[0] < 10 or img.shape[1] < 10 or\n",
        "        img.shape[2] not in [1, 3]\n",
        "    ):\n",
        "        print(f\"Skipping invalid image at index {i} with shape {img.shape}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        img_pil = Image.fromarray(img.squeeze() if img.shape[2] == 1 else img)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to convert image at index {i} with shape {img.shape}: {e}\")\n",
        "        continue\n",
        "\n",
        "    img_resized = img_pil.resize((256, 256))\n",
        "    img_resized_np = np.array(img_resized)\n",
        "\n",
        "    with gzip.open(f\"places_images_only/sample_{i}.pt.gz\", 'wb') as f:\n",
        "        pickle.dump({'images': img_resized_np}, f)"
      ],
      "metadata": {
        "id": "aitwm5Iiad6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"coco_images_only\", exist_ok=True)\n",
        "\n",
        "coco_samples = 20000\n",
        "\n",
        "\n",
        "for i, sample in enumerate(tqdm(coco_train_loader, total=coco_samples, desc=\"Saving resized COCO images\")):\n",
        "    if i >= coco_samples:\n",
        "        break\n",
        "\n",
        "    img = sample['images']\n",
        "\n",
        "    if isinstance(img, torch.Tensor):\n",
        "        img = img.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "    img = img.astype(np.uint8)\n",
        "\n",
        "    if (\n",
        "        img.ndim != 3 or\n",
        "        img.shape[0] < 10 or img.shape[1] < 10 or\n",
        "        img.shape[2] not in [1, 3]\n",
        "    ):\n",
        "        print(f\"Skipping invalid image at index {i} with shape {img.shape}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        img_pil = Image.fromarray(img.squeeze() if img.shape[2] == 1 else img)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to convert image at index {i} with shape {img.shape}: {e}\")\n",
        "        continue\n",
        "\n",
        "    img_resized = img_pil.resize((256, 256))\n",
        "    img_resized_np = np.array(img_resized)\n",
        "\n",
        "    with gzip.open(f\"coco_images_only/sample_{i}.pt.gz\", 'wb') as f:\n",
        "        pickle.dump({'images': img_resized_np}, f)"
      ],
      "metadata": {
        "id": "1uw4eO5JagSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coco_ds_offset = deeplake.query('SELECT * FROM \"hub://activeloop/coco-train\" LIMIT 20000 OFFSET 20000')\n",
        "coco_train_loader_2 = coco_ds_offset.pytorch()"
      ],
      "metadata": {
        "id": "aKr_Uygjajkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_index = 20000\n",
        "coco_samples = 20000\n",
        "\n",
        "for i, sample in enumerate(tqdm(coco_train_loader_2, total=coco_samples, desc=\"Saving resized COCO images\")):\n",
        "    if i >= coco_samples:\n",
        "        break\n",
        "\n",
        "    img = sample['images']\n",
        "\n",
        "    if isinstance(img, torch.Tensor):\n",
        "        img = img.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "    img = img.astype(np.uint8)\n",
        "\n",
        "    if (\n",
        "        img.ndim != 3 or\n",
        "        img.shape[0] < 10 or img.shape[1] < 10 or\n",
        "        img.shape[2] not in [1, 3]\n",
        "    ):\n",
        "        print(f\"Skipping invalid image at index {i} with shape {img.shape}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        img_pil = Image.fromarray(img.squeeze() if img.shape[2] == 1 else img)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to convert image at index {i} with shape {img.shape}: {e}\")\n",
        "        continue\n",
        "\n",
        "    img_resized = img_pil.resize((256, 256))\n",
        "    img_resized_np = np.array(img_resized)\n",
        "\n",
        "    save_index = start_index + i\n",
        "    with gzip.open(f\"coco_images_only/sample_{save_index}.pt.gz\", 'wb') as f:\n",
        "        pickle.dump({'images': img_resized_np}, f)"
      ],
      "metadata": {
        "id": "NV_HuXy8ak-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coco_ds_offset = deeplake.query('SELECT * FROM \"hub://activeloop/coco-train\" LIMIT 10000 OFFSET 40000')\n",
        "coco_train_loader_3 = coco_ds_offset.pytorch()"
      ],
      "metadata": {
        "id": "NavMz3BzanrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_index = 40000\n",
        "coco_samples = 10000\n",
        "\n",
        "for i, sample in enumerate(tqdm(coco_train_loader_3, total=coco_samples, desc=\"Saving resized COCO images\")):\n",
        "    if i >= coco_samples:\n",
        "        break\n",
        "\n",
        "    img = sample['images']\n",
        "\n",
        "    if isinstance(img, torch.Tensor):\n",
        "        img = img.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "    img = img.astype(np.uint8)\n",
        "\n",
        "    if (\n",
        "        img.ndim != 3 or\n",
        "        img.shape[0] < 10 or img.shape[1] < 10 or\n",
        "        img.shape[2] not in [1, 3]\n",
        "    ):\n",
        "        print(f\"Skipping invalid image at index {i} with shape {img.shape}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        img_pil = Image.fromarray(img.squeeze() if img.shape[2] == 1 else img)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to convert image at index {i} with shape {img.shape}: {e}\")\n",
        "        continue\n",
        "\n",
        "    img_resized = img_pil.resize((256, 256))\n",
        "    img_resized_np = np.array(img_resized)\n",
        "\n",
        "    save_index = start_index + i\n",
        "    with gzip.open(f\"coco_images_only/sample_{save_index}.pt.gz\", 'wb') as f:\n",
        "        pickle.dump({'images': img_resized_np}, f)"
      ],
      "metadata": {
        "id": "3WKjy2Q6aoyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "places_dataset = ColorizationDataset(places205_ds, style_encoder=resnet_style, device=device)\n",
        "coco_dataset = ColorizationDataset(coco_ds, style_encoder=resnet_style, device=device)"
      ],
      "metadata": {
        "id": "-enBbofyarTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "BNCM3V8Kasf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset = torch.utils.data.ConcatDataset([places_dataset, coco_dataset])\n",
        "train_loader = DataLoader(combined_dataset, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "GLpdo8oHaued"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiskImageDataset(Dataset):\n",
        "    def __init__(self, cache_dir, num_samples):\n",
        "        self.cache_dir = cache_dir\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # return torch.load(os.path.join(self.cache_dir, f\"sample_{idx}.pt\"), weights_only=False)\n",
        "        path = os.path.join(self.cache_dir, f\"sample_{idx}.pt.gz\")\n",
        "        with gzip.open(path, 'rb') as f:\n",
        "            return pickle.load(f)"
      ],
      "metadata": {
        "id": "T4ZEiYfuavok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "places_disk = DiskImageDataset(\"places_images_only\", 20000)\n",
        "coco_disk = DiskImageDataset(\"coco_images_only\", 50000)\n",
        "\n",
        "places_dataset = ColorizationDataset(places_disk, style_encoder=resnet_style, device=device)\n",
        "coco_dataset = ColorizationDataset(coco_disk, style_encoder=resnet_style, device=device)"
      ],
      "metadata": {
        "id": "7dI9Fvwbaw_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset = torch.utils.data.ConcatDataset([places_dataset, coco_dataset])"
      ],
      "metadata": {
        "id": "Uy7TFf79ay3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "places_visualization = ColorizationDataset(places_disk, style_encoder=resnet_style, device=device, test=True)\n",
        "coco_visualization = ColorizationDataset(coco_disk, style_encoder=resnet_style, device=device, test=True)"
      ],
      "metadata": {
        "id": "LveeLJqVa0T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "train_size = int(0.9 * len(combined_dataset))\n",
        "val_size = int(0.05 * len(combined_dataset))\n",
        "test_size = len(combined_dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(combined_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "id": "8F6wwmn7a1Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_loader))\n",
        "print(len(val_loader))\n",
        "print(len(test_loader))"
      ],
      "metadata": {
        "id": "PhuMdzK4a1_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.amp import GradScaler, autocast"
      ],
      "metadata": {
        "id": "MEy19_Hca3ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ColorizationNet(device=device)\n",
        "loss_fn = CombinedLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "scaler = GradScaler('cuda')"
      ],
      "metadata": {
        "id": "QjENaJzsa4ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "\n",
        "deeplab = deeplabv3_resnet50(pretrained=True).eval().to(device)"
      ],
      "metadata": {
        "id": "F9jE2WjQa5cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
        "mask_rcnn = maskrcnn_resnet50_fpn(pretrained=True).eval().to(device)"
      ],
      "metadata": {
        "id": "CXvvaXpDa6Tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_instance_feats(model, image_tensor, device, threshold=0.5):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image_tensor = image_tensor.to(device)\n",
        "        detections = model([image_tensor])[0]\n",
        "\n",
        "        if \"masks\" not in detections or len(detections[\"masks\"]) == 0:\n",
        "            return torch.zeros((1, 256, 256)).to(device)\n",
        "\n",
        "        masks = detections[\"masks\"]\n",
        "        masks = masks.squeeze(1)\n",
        "\n",
        "        binary_masks = (masks > threshold).float()\n",
        "        instance_map = binary_masks.sum(0, keepdim=True)\n",
        "\n",
        "        return instance_map"
      ],
      "metadata": {
        "id": "DtBYCKrSa7gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lab_to_rgb_opencv(L, ab):\n",
        "    L = (L * 255).astype(np.uint8)\n",
        "    ab = ((ab * 128) + 128).astype(np.uint8)\n",
        "    lab = np.concatenate([L, ab], axis=2)\n",
        "    rgb = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
        "    return np.clip(rgb, 0, 255).astype(np.uint8)"
      ],
      "metadata": {
        "id": "5ra-AkVsa8xK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "def compute_metrics(pred_ab, target_ab, input_l):\n",
        "    pred_ab_np = pred_ab.detach().cpu().permute(1, 2, 0).numpy()\n",
        "    target_ab_np = target_ab.detach().cpu().permute(1, 2, 0).numpy()\n",
        "    L_np = input_l.detach().cpu().squeeze(0).numpy()[..., np.newaxis]\n",
        "\n",
        "    pred_rgb = lab_to_rgb_opencv(L_np, pred_ab_np)\n",
        "    target_rgb = lab_to_rgb_opencv(L_np, target_ab_np)\n",
        "\n",
        "    psnr_val = psnr(target_rgb, pred_rgb, data_range=255)\n",
        "    ssim_val = ssim(target_rgb, pred_rgb, channel_axis=-1, data_range=255)\n",
        "\n",
        "    return psnr_val, ssim_va"
      ],
      "metadata": {
        "id": "067cPhM0a9tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "SAVE_DIR = '/content/drive/MyDrive/colorization_models/'\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "YNQ7zA-sa--E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "def train(model, loss_fn, train_loader, val_loader, optimizer, device, epochs=10):\n",
        "    model.to(device)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 3\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=True)\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            input_l = batch['L'].to(device)\n",
        "            target_ab = batch['ab'].to(device)\n",
        "            input_l_3ch = input_l.repeat(1, 3, 1, 1) if input_l.shape[1] == 1 else input_l\n",
        "\n",
        "            with torch.no_grad():\n",
        "                seg_output = deeplab(input_l_3ch)['out']\n",
        "                segmap = torch.argmax(seg_output, dim=1, keepdim=True).float()\n",
        "\n",
        "                instance_feats = torch.stack([\n",
        "                    extract_instance_feats(mask_rcnn, img.cpu(), device)\n",
        "                    for img in input_l_3ch\n",
        "                ]).to(device)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            with autocast():\n",
        "                pred_ab, color_query = model(input_l_3ch, segmap, instance_feats=instance_feats, style_feats=style_feats)\n",
        "                _, target_query = model(input_l_3ch, segmap, instance_feats=instance_feats)\n",
        "                loss = loss_fn(pred_ab, target_ab, input_l, color_query, target_query)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            avg_loss = running_loss / (progress_bar.n + 1)\n",
        "            progress_bar.set_postfix(train_loss=avg_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        psnr_list = []\n",
        "        ssim_list = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_l = batch['L'].to(device)\n",
        "                target_ab = batch['ab'].to(device)\n",
        "                input_l_3ch = input_l.repeat(1, 3, 1, 1) if input_l.shape[1] == 1 else input_l\n",
        "\n",
        "                seg_output = deeplab(input_l_3ch)['out']\n",
        "                segmap = torch.argmax(seg_output, dim=1, keepdim=True).float()\n",
        "\n",
        "                instance_feats = torch.stack([\n",
        "                    extract_instance_feats(mask_rcnn, img.cpu(), device)\n",
        "                    for img in input_l_3ch\n",
        "                ]).to(device)\n",
        "\n",
        "                pred_ab, color_query = model(input_l_3ch, segmap, instance_feats=instance_feats, style_feats=style_feats)\n",
        "                _, target_query = model(input_l_3ch, segmap, instance_feats=instance_feats)\n",
        "\n",
        "                loss = loss_fn(pred_ab, target_ab, input_l, color_query, target_query)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                pred_ab_upsampled = F.interpolate(pred_ab, size=input_l.shape[2:], mode='bilinear', align_corners=False)\n",
        "                for i in range(pred_ab_upsampled.size(0)):\n",
        "                    psnr_val, ssim_val = compute_metrics(pred_ab_upsampled[i], target_ab[i], input_l[i])\n",
        "                    psnr_list.append(psnr_val)\n",
        "                    ssim_list.append(ssim_val)\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        avg_psnr = sum(psnr_list) / len(psnr_list)\n",
        "        avg_ssim = sum(ssim_list) / len(ssim_list)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Validation Loss: {avg_val_loss:.4f}, PSNR: {avg_psnr:.2f}, SSIM: {avg_ssim:.4f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'best_model_epoch.pth'))\n",
        "            checkpoint = {\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scaler_state_dict': scaler.state_dict(),\n",
        "                'best_val_loss': best_val_loss,\n",
        "                'epochs_no_improve': epochs_no_improve\n",
        "            }\n",
        "            torch.save(checkpoint, os.path.join(SAVE_DIR, 'checkpoint_epoch.pth'))\n",
        "            print(\"Saved new best model.\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"No improvement for {epochs_no_improve} epoch(s).\")\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break"
      ],
      "metadata": {
        "id": "am8Yr5wxbAH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, loss_fn, train_loader, val_loader, optimizer, device, epochs=5)"
      ],
      "metadata": {
        "id": "jGOoB2tPbCGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_test(model, loss_fn, test_loader, device):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        psnr_list = []\n",
        "        ssim_list = []\n",
        "\n",
        "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "            input_l = batch['L'].to(device)\n",
        "            target_ab = batch['ab'].to(device)\n",
        "            input_l_3ch = input_l.repeat(1, 3, 1, 1)\n",
        "\n",
        "            seg_output = deeplab(input_l_3ch)['out']\n",
        "            segmap = torch.argmax(seg_output, dim=1, keepdim=True).float()\n",
        "\n",
        "            instance_feats = torch.stack([\n",
        "                extract_instance_feats(mask_rcnn, img.cpu(), device)\n",
        "                for img in input_l_3ch\n",
        "            ]).to(device)\n",
        "\n",
        "            _, target_query = model(input_l_3ch, segmap, instance_feats=instance_feats)\n",
        "\n",
        "            pred_ab, color_query = model(input_l_3ch, segmap, instance_feats=instance_feats, style_feats=style_feats)\n",
        "\n",
        "            loss = loss_fn(pred_ab, target_ab, input_l, color_query, target_query)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            pred_ab_upsampled = F.interpolate(pred_ab, size=input_l.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "            for i in range(pred_ab_upsampled.size(0)):\n",
        "                psnr_val, ssim_val = compute_metrics(\n",
        "                    pred_ab_upsampled[i], target_ab[i], input_l[i]\n",
        "                )\n",
        "                psnr_list.append(psnr_val)\n",
        "                ssim_list.append(ssim_val)\n",
        "\n",
        "\n",
        "    avg_psnr = sum(psnr_list) / len(psnr_list)\n",
        "    avg_ssim = sum(ssim_list) / len(ssim_list)\n",
        "    print(f\"Validation PSNR: {avg_psnr:.2f}, SSIM: {avg_ssim:.4f}\")\n",
        "    avg_test_loss = total_loss / len(test_loader)\n",
        "    print(f\"Test Loss: {avg_test_loss:.4f}\")"
      ],
      "metadata": {
        "id": "XtZ0tIm0bGeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_test(model, loss_fn, test_loader, device)"
      ],
      "metadata": {
        "id": "A38z1xUibHmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "\n",
        "def test_and_visualize(model, dataset, device, num_samples=30):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        sample = dataset[i]\n",
        "        input_l = sample['L'].unsqueeze(0).to(device)\n",
        "        target_ab = sample['ab'].unsqueeze(0).to(device)\n",
        "\n",
        "        input_l_3ch = input_l.repeat(1, 3, 1, 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            seg_output = deeplab(input_l_3ch)['out']\n",
        "            segmap = torch.argmax(seg_output, dim=1, keepdim=True).float().to(device)\n",
        "\n",
        "            img_for_detection = input_l_3ch[0].cpu()\n",
        "            instance_feats = extract_instance_feats(mask_rcnn, img_for_detection, device)\n",
        "            instance_feats = instance_feats.unsqueeze(0)\n",
        "\n",
        "            pred_ab, _ = model(input_l_3ch, segmap, instance_feats=instance_feats)\n",
        "\n",
        "        pred_ab_upsampled = F.interpolate(pred_ab, size=input_l.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        L_np = input_l.squeeze().cpu().numpy()[..., np.newaxis]\n",
        "        pred_ab_np = pred_ab_upsampled.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
        "        target_ab_np = target_ab.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "        # print(pred_ab.min().item(), pred_ab.max().item())\n",
        "\n",
        "        pred_rgb = lab_to_rgb_opencv(L_np, pred_ab_np)\n",
        "        target_rgb = lab_to_rgb_opencv(L_np, target_ab_np)\n",
        "        gray_img = (L_np * 255).astype(np.uint8).squeeze()\n",
        "\n",
        "        axes[i, 0].imshow(gray_img, cmap='gray')\n",
        "        axes[i, 0].set_title('Grayscale Input')\n",
        "        axes[i, 1].imshow(pred_rgb)\n",
        "        axes[i, 1].set_title('Predicted Colorization')\n",
        "        axes[i, 2].imshow(target_rgb)\n",
        "        axes[i, 2].set_title('Ground Truth')\n",
        "\n",
        "        for ax in axes[i]:\n",
        "            ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ZVgVoLFHbJMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_and_visualize(model, coco_visualization, device)"
      ],
      "metadata": {
        "id": "lHj1JPXqbKF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "def preprocess_external_image(path):\n",
        "    img = Image.open(path).convert(\"RGB\").resize((256, 256))\n",
        "    img_np = np.array(img)\n",
        "\n",
        "    lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB).astype(np.float32)\n",
        "    L = lab[:, :, 0:1] / 255.0\n",
        "    ab = (lab[:, :, 1:3] - 128) / 128.0\n",
        "\n",
        "    L_tensor = torch.from_numpy(L).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
        "    ab_tensor = torch.from_numpy(ab).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
        "\n",
        "    return L_tensor, ab_tensor, img_np\n"
      ],
      "metadata": {
        "id": "raRpT4ETbMrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_external_images(model, paths, device):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    fig, axes = plt.subplots(len(paths), 3, figsize=(12, 4 * len(paths)))\n",
        "\n",
        "    for i, path in enumerate(paths):\n",
        "        input_l, target_ab, original_img = preprocess_external_image(path)\n",
        "        input_l_3ch = input_l.repeat(1, 3, 1, 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            seg_output = deeplab(input_l_3ch)['out']\n",
        "            segmap = torch.argmax(seg_output, dim=1, keepdim=True).float().to(device)\n",
        "\n",
        "            instance_feats = extract_instance_feats(mask_rcnn, input_l_3ch[0].cpu(), device).unsqueeze(0)\n",
        "\n",
        "            pred_ab, _ = model(input_l_3ch, segmap, instance_feats=instance_feats)\n",
        "\n",
        "            pred_ab_upsampled = F.interpolate(pred_ab, size=input_l.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        L_np = input_l.squeeze().cpu().numpy()[..., np.newaxis]\n",
        "        pred_ab_np = pred_ab_upsampled.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
        "        target_ab_np = target_ab.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "        pred_rgb = lab_to_rgb_opencv(L_np, pred_ab_np)\n",
        "        target_rgb = lab_to_rgb_opencv(L_np, target_ab_np)\n",
        "        gray_img = (L_np * 255).astype(np.uint8).squeeze()\n",
        "\n",
        "        axes[i, 0].imshow(gray_img, cmap='gray')\n",
        "        axes[i, 0].set_title(f\"{path} - Grayscale\")\n",
        "        axes[i, 1].imshow(pred_rgb)\n",
        "        axes[i, 1].set_title(\"Predicted Colorization\")\n",
        "        axes[i, 2].imshow(original_img)\n",
        "        axes[i, 2].set_title(\"Original RGB\")\n",
        "\n",
        "        for ax in axes[i]:\n",
        "            ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "cb_1k-p4bNlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = [\"1.png\", \"2.png\", \"3.png\", \"4.png\", \"5.png\"]\n",
        "test_external_images(model, image_paths, device)"
      ],
      "metadata": {
        "id": "l1rctsYTbPGj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}